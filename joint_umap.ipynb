{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalizing TODOs\n",
    "    First pass writing\n",
    "    Fill in plots\n",
    "    Iterate with Sam\n",
    "    Do colab\n",
    "    (Find circuits)\n",
    "    Finish and submit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn.functional as F\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from dictionary_learning import AutoEncoder\n",
    "from dictionary_learning.buffer import ActivationBuffer\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXConfig {\n",
       "  \"_name_or_path\": \"EleutherAI/pythia-70m-deduped\",\n",
       "  \"architectures\": [\n",
       "    \"GPTNeoXForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout\": 0.0,\n",
       "  \"hidden_size\": 512,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 2048,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"gpt_neox\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rotary_emb_base\": 10000,\n",
       "  \"rotary_pct\": 0.25,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.35.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_parallel_residual\": true,\n",
       "  \"vocab_size\": 50304\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda:0\"\n",
    "DEBUGGING = False\n",
    "\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = {'validate' : True, 'scan' : True}\n",
    "else:\n",
    "    tracer_kwargs = {'validate' : False, 'scan' : False}\n",
    "\n",
    "model = LanguageModel(\n",
    "    \"EleutherAI/pythia-70m-deduped\",\n",
    "    device_map = DEVICE,\n",
    "    dispatch = True,\n",
    ")\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MLP submodules\n",
    "D_MODEL = model.config.hidden_size\n",
    "DICT_ID = 10\n",
    "D_SAE = 64 * D_MODEL\n",
    "\n",
    "# Load for chosen layers\n",
    "LAYERS = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "submodules = {f'mlp{l}': model.gpt_neox.layers[l].mlp for l in LAYERS}\n",
    "dictionaries = {}\n",
    "for l, name in zip(LAYERS, submodules.keys()):\n",
    "    ae = AutoEncoder(D_MODEL, D_SAE).to(DEVICE)\n",
    "    ae.load_state_dict(t.load(f'/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/mlp_out_layer{l}/{DICT_ID}_{D_SAE}/ae.pt'))\n",
    "    dictionaries[f'mlp{l}'] = ae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out dead features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zstandard import ZstdDecompressor\n",
    "import json\n",
    "import io\n",
    "\n",
    "# Load data from the pile\n",
    "data_path = '/share/data/datasets/pile/the-eye.eu/public/AI/pile/train/00.jsonl.zst'\n",
    "compressed_file = open(data_path, 'rb')\n",
    "dctx = ZstdDecompressor()\n",
    "reader = dctx.stream_reader(compressed_file)\n",
    "text_stream = io.TextIOWrapper(reader, encoding='utf-8')\n",
    "\n",
    "def generator():\n",
    "    for line in text_stream:\n",
    "        yield json.loads(line)['text']\n",
    "data_generator = generator()\n",
    "\n",
    "def tokenized_batch(generator, batch_size, ctx_len):\n",
    "    try:\n",
    "        texts = [next(generator) for _ in range(batch_size)]\n",
    "    except StopIteration:\n",
    "        raise StopIteration(\"End of data stream reached\")\n",
    "    \n",
    "    return model.tokenizer(\n",
    "        texts,\n",
    "        return_tensors='pt',\n",
    "        max_length=ctx_len,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "        )\n",
    "\n",
    "# buffer = ActivationBuffer(\n",
    "#     data,\n",
    "#     model,\n",
    "#     submodule,\n",
    "#     submodule_input_dim=D_MODEL,\n",
    "#     submodule_output_dim=D_SAE, # output dimension of the model component\n",
    "#     n_ctxs=1e4, # you can set this higher or lower dependong on your available memory\n",
    "#     ctx_len=128,\n",
    "#     io='out', # buffer will return batches of tensors of dimension = submodule's output dimension\n",
    "#     device='cuda:0' # doesn't have to be the same device that you train your autoencoder on\n",
    "# ) # buffer will return batches of tensors of dimension = submodule's output dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "CTX_LEN = 100\n",
    "\n",
    "cumulative_feature_act = {name: t.zeros((D_SAE), device='cpu') for name in submodules}\n",
    "\n",
    "for i in range(100):\n",
    "    input_batch = tokenized_batch(data_generator, batch_size=BATCH_SIZE, ctx_len=CTX_LEN).to(DEVICE)\n",
    "    activation_cache = {}\n",
    "    with t.no_grad(), model.trace(input_batch, **tracer_kwargs):\n",
    "        # out = model.gpt_neox.layers[0].mlp.output.save()\n",
    "        for name in submodules:\n",
    "            x = submodules[name].output\n",
    "            f = dictionaries[name].encode(x)\n",
    "            activation_cache[name] = f.save() # [BATCH_SIZE, CTX_LEN, D_SAE]\n",
    "\n",
    "    # The following loop causes a memory leak\n",
    "    for name in submodules:\n",
    "        sparse_acts = activation_cache[name].sum(dim=(0, 1)).to(\"cpu\")\n",
    "        cumulative_feature_act[name] += sparse_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_idxs = {}\n",
    "for name in submodules:\n",
    "    nonzero_idxs[name] = cumulative_feature_act[name].nonzero().flatten()\n",
    "    percent = nonzero_idxs[name].shape[0] / D_SAE\n",
    "    print(f\"{name}: {nonzero_idxs[name].shape[0]} nonzero activations ({percent:.2f} % alive)\")\n",
    "\n",
    "decoders = {}\n",
    "for name in submodules:\n",
    "    decoders[name] = dictionaries[name].decoder.weight.T # [D_SAE, D_MODEL]\n",
    "    decoders[name] = decoders[name][nonzero_idxs[name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarities of SAE decoder vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_stack = t.vstack((decoders['mlp1'], decoders['mlp2'], decoders['mlp3'], decoders['mlp4'], decoders['mlp5']))\n",
    "decoder_stack = F.normalize(decoder_stack, dim=1)\n",
    "\n",
    "cosine_sim = decoder_stack @ decoder_stack.T\n",
    "plt.imshow(cosine_sim.cpu().detach().numpy(), cmap='RdBu', interpolation='nearest', vmin=-1, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.title(\"Cosine similarity between decoders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_mlp5 = F.normalize(decoders['mlp5'], dim=1)\n",
    "cosine_sim = decoder_mlp5 @ decoder_mlp5.T\n",
    "plt.imshow(cosine_sim.cpu().detach().numpy(), cmap='RdBu', interpolation='nearest', vmin=-1, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.title(\"Cosine similarities of MLP5 decoder vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap(\n",
    "        vectors0,\n",
    "        vectors1 = None,\n",
    "        labels = ['vectors0', 'vectors1'],\n",
    "        hover_data0 = None,\n",
    "        hover_data1 = None,\n",
    "        highlight_features0 = [],\n",
    "        highlight_features1 = [],\n",
    "        title = 'UMAP embedding',\n",
    "        # UMAP parameters\n",
    "        n_neighbors=15,\n",
    "        metric='cosine',\n",
    "        min_dist=0.05,\n",
    "        n_components=2, # dimension of the UMAP embedding,\n",
    "        normalize = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fit a UMAP embedding of the dictionary features and return a plotly plot of the result.\n",
    "    vectorsX: sets of row vectors for UMAP embedding\n",
    "    \"\"\"\n",
    "    if vectors1 is None:\n",
    "        vectors_all = vectors0\n",
    "    else:\n",
    "        vectors_all = np.vstack([vectors0, vectors1])\n",
    "\n",
    "    if normalize:\n",
    "        vectors_all = vectors_all / np.linalg.norm(vectors_all, axis=1)[:, None]\n",
    "\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        metric=metric,\n",
    "        min_dist=min_dist,\n",
    "        n_components=n_components,\n",
    "    )\n",
    "    embedding = reducer.fit_transform(vectors_all)\n",
    "\n",
    "    # DEBUGGING: Create random data in shape of the embedding\n",
    "    # embedding = np.random.rand(vectors_all.shape[0], n_components)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['umapX'] = embedding[:, 0]\n",
    "    df['umapY'] = embedding[:, 1]\n",
    "    if n_components == 3:\n",
    "        df['umapZ'] = embedding[:, 2]\n",
    "\n",
    "    if vectors1 is None:\n",
    "        df['hover_data'] = [f'feature {i}' for i in hover_data0]\n",
    "        df['label'] = [f'highlight_{labels[0]}' if h0 in highlight_features0 else labels[0] for h0 in hover_data0]\n",
    "    else:\n",
    "        df['hover_data'] = [f'feature {i}' for i in t.hstack([hover_data0, hover_data1])]\n",
    "        labels0 = [f'highlight_{labels[0]}' if h0 in highlight_features0 else labels[0] for h0 in hover_data0]\n",
    "        labels1 = [f'highlight_{labels[1]}' if h1 in highlight_features1 else labels[1] for h1 in hover_data1]\n",
    "        df['label'] = labels0 + labels1\n",
    "        print(df.label.unique())\n",
    "\n",
    "    if n_components == 2:\n",
    "        fig = px.scatter(df, x='umapX', y='umapY', color='label', opacity=0.5, hover_data='hover_data')\n",
    "    elif n_components == 3:\n",
    "        fig = px.scatter_3d(df, x='umapX', y='umapY', z='umapZ', color='label', opacity=0.5, hover_data='hover_data')\n",
    "    else:\n",
    "        raise ValueError(\"n_components must be 2 or 3\")\n",
    "    fig.update_layout(title=title)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Umap for mlp_5 decoder\n",
    "plot_umap(\n",
    "    decoders[\"mlp5\"].cpu().detach().numpy(), \n",
    "    labels = [\"mlp5\"],\n",
    "    hover_data0 = [f'feature {i}' for i in nonzero_idxs[\"mlp5\"]],\n",
    "    title = \"UMAP of mlp5_decoder\",\n",
    "    normalize = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_umap(\n",
    "    decoders[\"mlp4\"].cpu().detach().numpy(), \n",
    "    decoders[\"mlp5\"].cpu().detach().numpy(), \n",
    "    labels = [\"mlp4\", \"mlp5\"],\n",
    "    hover_data0 = nonzero_idxs[\"mlp4\"],\n",
    "    hover_data1 = nonzero_idxs[\"mlp5\"],\n",
    "    highlight_features0=[20006],\n",
    "    highlight_features1=[4015, 15980],\n",
    "    title = \"Joint UMAP of mlp4_decoder and mlp5_decoder\",\n",
    "    normalize = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Umap for mlp_2 decoder\n",
    "plot_umap(\n",
    "    decoders['mlp2'].cpu().detach().numpy(), \n",
    "    labels = [\"mlp2\"],\n",
    "    hover_data0 = [f'feature {i}' for i in nonzero_idxs[\"mlp2\"]],\n",
    "    title = \"UMAP of mlp2_decoder\",\n",
    "    normalize = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Umap for mlp_3 decoder\n",
    "plot_umap(\n",
    "    decoders[\"mlp3\"].cpu().detach().numpy(), \n",
    "    labels = [\"mlp3\"],\n",
    "    hover_data0 = [f'feature {i}' for i in nonzero_idxs[\"mlp3\"]],\n",
    "    title = \"UMAP of mlp3_decoder\",\n",
    "    normalize = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_umap(\n",
    "    decoders[\"mlp2\"].cpu().detach().numpy(), \n",
    "    decoders[\"mlp3\"].cpu().detach().numpy(), \n",
    "    labels = [\"mlp2\", \"mlp3\"],\n",
    "    hover_data0 = nonzero_idxs[\"mlp2\"],\n",
    "    hover_data1 = nonzero_idxs[\"mlp3\"],\n",
    "    highlight_features0=[20006],\n",
    "    highlight_features1=[4015, 15980],\n",
    "    title = \"Joint UMAP of mlp2_decoder and mlp3_decoder\",\n",
    "    normalize = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = \"\"\"28138\n",
    "25379\n",
    "26215\n",
    "21676\n",
    "21618\n",
    "16166\n",
    "118\n",
    "5235\n",
    "3387\n",
    "5160\n",
    "5940\n",
    "7047\n",
    "27799\n",
    "13281\n",
    "30067\n",
    "13420\n",
    "19904\n",
    "29973\n",
    "\"\"\"\n",
    "\n",
    "feats = [int(f) for f in feats.split('\\n') if f]\n",
    "print(f\"Selected features: {feats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "def open_neuronpedia(feature_id: int, layer: int = 0):\n",
    "    path_to_html = f\"https://www.neuronpedia.org/pythia-70m-deduped/{layer}-mlp-sm/{feature_id}\"\n",
    "\n",
    "    print(f\"Feature {feature_id}\")\n",
    "    webbrowser.open_new_tab(path_to_html)\n",
    "\n",
    "for feature in feats:\n",
    "    open_neuronpedia(feature, layer=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completeness of this set of features the ability to predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'D_MODEL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m resids \u001b[38;5;241m=\u001b[39m [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mgpt_neox\u001b[38;5;241m.\u001b[39mlayers]\n\u001b[1;32m      7\u001b[0m dictionaries \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 8\u001b[0m ae \u001b[38;5;241m=\u001b[39m AutoEncoder(\u001b[43mD_MODEL\u001b[49m, D_SAE)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      9\u001b[0m ae\u001b[38;5;241m.\u001b[39mload_state_dict(t\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/embed/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDICT_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mD_SAE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/ae.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     10\u001b[0m dictionaries[embed] \u001b[38;5;241m=\u001b[39m ae\n",
      "\u001b[0;31mNameError\u001b[0m: name 'D_MODEL' is not defined"
     ]
    }
   ],
   "source": [
    "# Load submodules and dictionaries\n",
    "embed = model.gpt_neox.embed_in\n",
    "attns = [layer.attention for layer in model.gpt_neox.layers]\n",
    "mlps = [layer.mlp for layer in model.gpt_neox.layers]\n",
    "resids = [layer for layer in model.gpt_neox.layers]\n",
    "\n",
    "dictionaries = {}\n",
    "ae = AutoEncoder(D_MODEL, D_SAE).to(DEVICE)\n",
    "ae.load_state_dict(t.load(f'/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/embed/{DICT_ID}_{D_SAE}/ae.pt'))\n",
    "dictionaries[embed] = ae\n",
    "for i in range(len(model.gpt_neox.layers)):\n",
    "    ae = AutoEncoder(D_MODEL, D_SAE).to(DEVICE)\n",
    "    ae.load_state_dict(t.load(f'/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/attn_out_layer{i}/{DICT_ID}_{D_SAE}/ae.pt'))\n",
    "    dictionaries[attns[i]] = ae\n",
    "\n",
    "    ae = AutoEncoder(D_MODEL, D_SAE).to(DEVICE)\n",
    "    ae.load_state_dict(t.load(f'/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/mlp_out_layer{i}/{DICT_ID}_{D_SAE}/ae.pt'))\n",
    "    dictionaries[mlps[i]] = ae\n",
    "\n",
    "    ae = AutoEncoder(D_MODEL, D_SAE).to(DEVICE)\n",
    "    ae.load_state_dict(t.load(f'/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/resid_out_layer{i}/{DICT_ID}_{D_SAE}/ae.pt'))\n",
    "    dictionaries[resids[i]] = ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/can/.conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "         0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
       "        [1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "         0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = \"\"\"\n",
    "An astronaut thought about an acorn growing into an oak tree in an invisible wire. An economist wrote an article about an apple that could make an espresso in various unexpected combinations. An ice cream vendor dreamed of an artifact at an unexpected depth. An envelope containing an unusual color was left on an empty seat in an altruist spell on an adventure.\n",
    "An idol decided one day that an alien was necessary for an intergalactic bar. An unusual occurrence happened at an intersection involving an espresso in an empty theater. An actor rehearsed an awkward hour at an improbable depth. An owl in an apple rolled across an uneven table or hooted in an old shop.\n",
    "An acrobat performed an incredible stunt on an old book laying open with an intriguing letter. An optometrist invented an orchestra that played an overture in an enlightened hall. An engineer on vacation found an umbrella or an altruist involved with an ice truck. An archaeologist dug up an array of peculiar stories on an old map.\n",
    "An avatar repeated an action endlessly due to an unexpected error in an algorithm. An artist painted an apple with an elaborate process in an economic theory. An odd silence filled the room as an elusive fox disturbed an opera singer. An orchestra played without an audience, or an opera singer performed on an empty seat\n",
    "\"\"\"\n",
    "\n",
    "prompts = prompts.split(\". \")\n",
    "prompts = [p.strip(\"\\n\") for p in prompts if p]\n",
    "\n",
    "tokenized_prompts = model.tokenizer(prompts, return_tensors='pt', max_length=64, padding=True)\n",
    "model.tokenizer.encode(\" an\")\n",
    "\n",
    "an_tokens = [\" An\", \"An\", \" an\", \"an\"] # The prompt was explicitly designed to contain these tokens only as articles\n",
    "an_token_ids = [i[0] for i in model.tokenizer(an_tokens).input_ids]\n",
    "an_token_ids\n",
    "\n",
    "token_after_article_mask = t.zeros_like(tokenized_prompts['input_ids'])\n",
    "for token_id in an_token_ids:\n",
    "    token_after_article_mask += (tokenized_prompts['input_ids'] == token_id).int()\n",
    "token_after_article_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 39, 50304])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metric for the normal forward pass of the model\n",
    "with model.trace(tokenized_prompts, **tracer_kwargs) as trace:\n",
    "    clean_logits = model.output.save()\n",
    "\n",
    "clean_logits[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 39, 50304])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-inf, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perplexity_on_masked_tokens(logits, token_after_article_mask):\n",
    "    # Calculate perplexity on tokens that follow the article token\n",
    "    # Mask the tokens that follow the article token\n",
    "    log_probs = t.log(F.softmax(logits, dim=-1))\n",
    "    return log_probs[token_after_article_mask].sum()\n",
    "    \n",
    "perplexity_on_masked_tokens(clean_logits[0], token_after_article_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which other features have a high attribution score for my metric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
